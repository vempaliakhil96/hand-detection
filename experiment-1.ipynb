{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import IPython\n",
    "import time\n",
    "from io import BytesIO      \n",
    "import numpy as np\n",
    "import PIL\n",
    "from IPython.display import display\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(cam):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    #flip image for natural viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    #frame = modify_frame(frame)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def array_to_image(a, fmt='jpeg'):\n",
    "    #Create binary stream object\n",
    "    f = BytesIO()\n",
    "\n",
    "    #Convert array to binary stream object\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "\n",
    "    return IPython.display.Image(data=f.getvalue())\n",
    "\n",
    "def process_image(frame):\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # define range of skin color in HSV\n",
    "    lower_skin = np.array([0, 36, 0], dtype=np.uint8)\n",
    "    upper_skin = np.array([57, 158, 255], dtype=np.uint8)\n",
    "    # extract skin colur image\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    # extrapolate the hand to fill dark spots within\n",
    "    frame = cv2.dilate(mask, kernel, iterations=4)\n",
    "    # blur the image\n",
    "    return frame\n",
    "\n",
    "def segment_hand_from_image(frame):\n",
    "    blurValue = 41  # GaussianBlur parameter\n",
    "    frame = cv2.bilateralFilter(frame, 5, 50, 100)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    img = frame\n",
    "    img = cv2.GaussianBlur(img, (blurValue, blurValue), 0)\n",
    "    img_copy = copy.deepcopy(img)\n",
    "    img = process_image(img)\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    if len(contours) != 0:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(img_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        roi = img_copy[y: y + h, x: x + w]\n",
    "        roi = process_image(roi)\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = \"images/\"\n",
    "gestures = ['high_five', 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_open_hand = path_to_images+gestures[1]\n",
    "image_list = [itr for itr in os.listdir(dir_open_hand) if itr[0]!=\".\"]\n",
    "image_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for image_name in image_list:\n",
    "    img = cv2.imread(dir_open_hand+\"/\"+image_name)\n",
    "    img = cv2.resize(img, image_size)\n",
    "    if 'image_shape' in globals():\n",
    "        pass\n",
    "    else:\n",
    "        image_shape = img.shape\n",
    "    X.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch in datagen.flow(X, batch_size=2,save_to_dir=dir_open_hand, save_prefix='generated', save_format='jpeg'):\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = \"images/\"\n",
    "gestures = {'high_five':1, 'null':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for gesture in gestures.keys():\n",
    "    path_ = path_to_images + gesture + \"/\"\n",
    "    for image_path in os.listdir(path_):\n",
    "        if image_path[0] == \".\":\n",
    "            continue\n",
    "        y.append(gestures[gesture])\n",
    "        img = cv2.imread(path_+image_path)\n",
    "        try:\n",
    "            img = img.resize(img, image_size)\n",
    "        except:\n",
    "            pass\n",
    "        X.append(img)\n",
    "        \"\"\"\n",
    "        if \"generated\" in image_path:\n",
    "            os.remove(path_+image_path)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr in range(len(X)):\n",
    "    if X[itr].shape[1:3] != (224, 224, 3):\n",
    "        X[itr] = cv2.resize(X[itr], image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = to_categorical(y, num_classes=2, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "mdl = applications.mobilenet_v2.MobileNetV2(input_shape=X.shape[1:], include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, MaxPool1D, MaxPooling2D, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "input_shape=X.shape[1:]\n",
    "model = Sequential()\n",
    "model.add(mdl)\n",
    "model.add(MaxPooling2D(pool_size=(5, 5), strides=5, padding='valid', data_format=None))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "527/527 [==============================] - 73s 138ms/step - loss: 1.2234 - accuracy: 0.5294\n",
      "Epoch 2/5\n",
      "527/527 [==============================] - 81s 153ms/step - loss: 0.8278 - accuracy: 0.6945\n",
      "Epoch 3/5\n",
      "527/527 [==============================] - 84s 159ms/step - loss: 0.5584 - accuracy: 0.7970\n",
      "Epoch 4/5\n",
      "527/527 [==============================] - 81s 153ms/step - loss: 0.4322 - accuracy: 0.8539\n",
      "Epoch 5/5\n",
      "527/527 [==============================] - 78s 149ms/step - loss: 0.3494 - accuracy: 0.8994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x143e32da0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 20, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def get_classification_report(y_test, y_predict):\n",
    "    \"\"\"\n",
    "    Gives out classification report for predictions of a model\n",
    "    \"\"\"\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "    print(\"\\n\\nClassification Report\")\n",
    "    print(classification_report(y_test, y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[91,  0],\n",
       "        [ 0, 41]],\n",
       "\n",
       "       [[76,  5],\n",
       "        [11, 40]],\n",
       "\n",
       "       [[81, 11],\n",
       "        [ 5, 35]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "cap_region_x_begin = 0.5  # start point/total width\n",
    "cap_region_y_end = 0.8  # start point/total width\n",
    "cam = cv2.VideoCapture(0)\n",
    "video_width = cam.get(3)\n",
    "video_height = cam.get(4)\n",
    "d = IPython.display.display(\"\", display_id=1)\n",
    "d2 = IPython.display.display(\"\", display_id=2)\n",
    "gestures = {'open_hand':1, 'closed_fist':2, 'null':0}\n",
    "inv_gestures = {1:'open_hand', 2:'close_fist', 0:'null'}\n",
    "while True:\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        frame = get_frame(cam)\n",
    "        roi = segment_hand_from_image(frame)\n",
    "        cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 0), (frame.shape[1], int(cap_region_y_end * frame.shape[0])), (0, 255, 0), 2)\n",
    "        roi = cv2.resize(roi, image_size)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        roi_copy = copy.deepcopy(roi)\n",
    "        roi = np.stack((roi, roi_copy, roi_copy), axis=3)\n",
    "        prediction = model.predict_classes(roi)[0]\n",
    "        prediction = inv_gestures[prediction]\n",
    "        \n",
    "        im = array_to_image(frame)\n",
    "        \n",
    "\n",
    "        d.update(im)\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        s = f\"\"\"{int(1/(t2-t1))} FPS and prediction is {prediction}\"\"\"\n",
    "        d2.update( IPython.display.HTML(s) )\n",
    "    except KeyboardInterrupt:\n",
    "        print()\n",
    "        cam.release()\n",
    "        IPython.display.clear_output()\n",
    "        print (\"Stream stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing train and test variables as pickle files to run in aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pack_variables = {'X': X, 'y': y}\n",
    "pickle.dump( pack_variables, open( \"variables.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2 exp-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "mdl = applications.mobilenet_v2.MobileNetV2(input_shape=X.shape[1:], include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, MaxPool1D, MaxPooling2D, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "input_shape=X.shape[1:]\n",
    "model = Sequential()\n",
    "model.add(mdl)\n",
    "model.add(MaxPooling2D(pool_size=(5, 5), strides=5, padding='valid', data_format=None))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 1280)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               655872    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 3,045,955\n",
      "Trainable params: 3,011,843\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "527/527 [==============================] - 72s 137ms/step - loss: 1.5498 - accuracy: 0.3776\n",
      "Epoch 2/20\n",
      "527/527 [==============================] - 61s 117ms/step - loss: 1.0383 - accuracy: 0.4744\n",
      "Epoch 3/20\n",
      "527/527 [==============================] - 62s 118ms/step - loss: 0.9767 - accuracy: 0.5617\n",
      "Epoch 4/20\n",
      "160/527 [========>.....................] - ETA: 43s - loss: 0.9195 - accuracy: 0.6500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-b690e3cd049b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/objectDetection/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 20, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = loaded_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = to_categorical(y_prediction, num_classes=3, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[91,  0],\n",
       "        [ 0, 41]],\n",
       "\n",
       "       [[81,  0],\n",
       "        [ 1, 50]],\n",
       "\n",
       "       [[91,  1],\n",
       "        [ 0, 40]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "cap_region_x_begin = 0.5  # start point/total width\n",
    "cap_region_y_end = 0.8  # start point/total width\n",
    "cam = cv2.VideoCapture(0)\n",
    "video_width = cam.get(3)\n",
    "video_height = cam.get(4)\n",
    "d = IPython.display.display(\"\", display_id=1)\n",
    "d2 = IPython.display.display(\"\", display_id=2)\n",
    "gestures = {'open_hand':1, 'closed_fist':2, 'null':0}\n",
    "inv_gestures = {1:'open_hand', 2:'close_fist', 0:'null'}\n",
    "while True:\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        frame = get_frame(cam)\n",
    "        roi = segment_hand_from_image(frame)\n",
    "        cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 0), (frame.shape[1], int(cap_region_y_end * frame.shape[0])), (0, 255, 0), 2)\n",
    "        roi = cv2.resize(roi, image_size)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        roi_copy = copy.deepcopy(roi)\n",
    "        \n",
    "        roi = np.stack((roi, roi_copy, roi_copy), axis=3)\n",
    "        prediction = loaded_model.predict_classes(roi)[0]\n",
    "        prediction = inv_gestures[prediction]\n",
    "        \n",
    "        im = array_to_image(roi.reshape(roi.shape[1:]))\n",
    "        \n",
    "\n",
    "        d.update(im)\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        s = f\"\"\"{int(1/(t2-t1))} FPS and prediction is {prediction}\"\"\"\n",
    "        d2.update( IPython.display.HTML(s) )\n",
    "    except KeyboardInterrupt:\n",
    "        print()\n",
    "        cam.release()\n",
    "        IPython.display.clear_output()\n",
    "        print (\"Stream stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_hand\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inv_gestures[y_prediction[2]])\n",
    "array_to_image(X_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_image(roi.reshape(roi.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 224, 224, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
